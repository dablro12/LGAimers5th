{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "from utils.EDA import vis_numeric_corr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 한글 처리\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "# 한글 폰트 경로 설정\n",
    "font_path = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
    "\n",
    "# 폰트 이름 얻어오기\n",
    "font_name = fm.FontProperties(fname=font_path).get_name()\n",
    "\n",
    "# 폰트 설정\n",
    "plt.rcParams['font.family'] = font_name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = os.getenv('TRAIN_DATA_PATH')\n",
    "TEST_PATH = os.getenv('TEST_DATA_PATH')\n",
    "\n",
    "train = pd.read_csv(TRAIN_PATH).drop(columns=['ID'])\n",
    "test = pd.read_csv(TEST_PATH)\n",
    "# vis_numeric_corr_matrix(train) #<--- for visualizing corr matrix\n",
    "print(f\"Column 일치 여부 : {train.columns.values.tolist()[:-1] == test.columns.values.tolist()}\")\n",
    "print(f\"Train Row 수 :{len(train)}\")\n",
    "print(f\"Test Row 수 :{len(test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.feature import preprocess\n",
    "pre_train = preprocess(train = train)\n",
    "pre_test = preprocess(train = test, validation= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결측 비율 확인\n",
    "pre_train.isnull().mean(), pre_test.isnull().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_train['총 임신 횟수'].value_counts()\n",
    "pre_train['총 출산 횟수'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.ft_generator import feature_engineering\n",
    "df_train = feature_engineering(pre_train)\n",
    "df_test = feature_engineering(pre_test)\n",
    "# 결측 비율\n",
    "missing_ratio = df_train.isnull().mean() * 100\n",
    "print(missing_ratio)\n",
    "print(len(df_train.columns))\n",
    "# vis_numeric_corr_matrix(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nan Inf Detect\n",
    "from utils.EDA import find_inf_nan_columns\n",
    "\n",
    "find_inf_nan_columns(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "LabelEncoder, OneHotEncoder, OrdinalEncoder, category_encoders.BinaryEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.model import bool_process, split_cat_num_cols\n",
    "from utils.data import check_non_numeric_values\n",
    "\n",
    "X = df_train.drop('임신 성공 여부', axis=1)\n",
    "y = df_train['임신 성공 여부']\n",
    "\n",
    "#########################\n",
    "# 2) 카테고리/연속형 컬럼 분리\n",
    "#########################\n",
    "X = bool_process(X)\n",
    "check_non_numeric_values(df=X)\n",
    "category_cols, continuous_cols = split_cat_num_cols(X, cat_unique_threshold=20)\n",
    "print(f\"범주형 자료 개수 : {len(category_cols)}\")\n",
    "print(f\"연속형 자료 개수 : {len(continuous_cols)}\")\n",
    "X[category_cols] = bool_process(X[category_cols])\n",
    "\n",
    "\n",
    "# Data split \n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, stratify=y, shuffle=True, random_state=123)\n",
    "\n",
    "X_test = df_test\n",
    "X_test[category_cols] = bool_process(X_test[category_cols])\n",
    "\n",
    "print('X_train.shape:', X_train.shape)\n",
    "print('X_val.shape:', X_valid.shape)\n",
    "print('y_train.shape:', y_train.shape)\n",
    "print('y_val.shape:', y_valid.shape)\n",
    "print('X_test.shape:', X_test.shape)\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "for col in category_cols:\n",
    "    le = LabelEncoder()\n",
    "    # 전체 데이터의 해당 컬럼을 합쳐서 fit\n",
    "    combined = pd.concat([X_train[col], X_valid[col], X_test[col]], axis=0).astype(str)\n",
    "    le.fit(combined)\n",
    "    \n",
    "    X_train[col] = le.transform(X_train[col].astype(str))\n",
    "    X_valid[col] = le.transform(X_valid[col].astype(str))\n",
    "    X_test[col]  = le.transform(X_test[col].astype(str))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 일반"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "from utils.metric import print_evaluation_metrics, evaluate_model\n",
    "from tabpfn import TabPFNClassifier\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier, TabNetRegressor\n",
    "import torch\n",
    "from tab_transformer_pytorch import FTTransformer\n",
    "from chempfn import ChemPFN\n",
    "\n",
    "from utils.model import *\n",
    "\n",
    "# # # 모델 정의\n",
    "# cat_model = CatBoostClassifier(\n",
    "#     iterations=700, learning_rate=0.03, depth=8, l2_leaf_reg=10,\n",
    "#     subsample=0.8, colsample_bylevel=0.8, random_strength=10,\n",
    "#     loss_function='Logloss', eval_metric='AUC', verbose=100\n",
    "# )\n",
    "\n",
    "# xgb_model = XGBClassifier(\n",
    "#     n_estimators=700, learning_rate=0.03, max_depth=7, min_child_weight=3,\n",
    "#     gamma=0.1, subsample=0.8, colsample_bytree=0.8, reg_alpha=0.1,\n",
    "#     reg_lambda=1.0, verbosity=1\n",
    "# )\n",
    "\n",
    "# lgbm_model = LGBMClassifier(\n",
    "#     n_estimators=700, learning_rate=0.03, max_depth=-1, num_leaves=64,\n",
    "#     min_child_samples=20, subsample=0.8, colsample_bytree=0.8,\n",
    "#     reg_alpha=0.1, reg_lambda=1.0, verbosity=1\n",
    "# )\n",
    "\n",
    "# ensemble_model = VotingClassifier(\n",
    "#     estimators=[('catboost', cat_model), ('xgboost', xgb_model), ('lightgbm', lgbm_model)],\n",
    "#     voting='soft', weights=[1, 1, 1]\n",
    "# )\n",
    "\n",
    "# tabpfn_model = TabPFNClassifier()\n",
    "\n",
    "# tabnet_model = TabNetClassifier()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "\n",
    "from ts3l.pl_modules import SCARFLightning\n",
    "from ts3l.utils.scarf_utils import SCARFDataset, SCARFConfig\n",
    "from ts3l.utils import TS3LDataModule\n",
    "from ts3l.utils.embedding_utils import IdentityEmbeddingConfig\n",
    "from ts3l.utils.backbone_utils import MLPBackboneConfig\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-1) train/valid split\n",
    "metric = \"roc_auc_score\"\n",
    "pretraining_head_dim = 1024\n",
    "output_dim = 2          # 이진분류 예시\n",
    "head_depth = 2\n",
    "dropout_rate = 0.04\n",
    "corruption_rate = 0.6\n",
    "device = 'gpu'\n",
    "batch_size = 128\n",
    "max_epochs = 20\n",
    "seed = 123\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_config = IdentityEmbeddingConfig(input_dim=X_train.shape[1])\n",
    "backbone_config = MLPBackboneConfig(input_dim=embedding_config.output_dim)\n",
    "\n",
    "config = SCARFConfig(\n",
    "    task=\"classification\",            # 분류 태스크\n",
    "    loss_fn=\"CrossEntropyLoss\",\n",
    "    metric=metric, metric_hparams={},\n",
    "    embedding_config=embedding_config,\n",
    "    backbone_config=backbone_config,\n",
    "    pretraining_head_dim=pretraining_head_dim,\n",
    "    output_dim=output_dim,\n",
    "    head_depth=head_depth,\n",
    "    dropout_rate=dropout_rate,\n",
    "    corruption_rate=corruption_rate,   # 자기지도 학습 시 데이터 일부 corrupted\n",
    "    random_seed = seed,\n",
    ")\n",
    "\n",
    "pl_scarf = SCARFLightning(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = SCARFDataset(X = X_train, unlabeled_data = X_test.drop(columns='ID'), config = config, continuous_cols=continuous_cols, category_cols=category_cols)\n",
    "valid_ds = SCARFDataset(X = X_valid, config = config, continuous_cols=continuous_cols, category_cols=category_cols)\n",
    "datamodule = TS3LDataModule(train_ds, valid_ds, batch_size=batch_size, train_sampler=\"random\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    accelerator=device,      # GPU 사용 가능하면 'gpu' or 'auto'\n",
    "    max_epochs=max_epochs,\n",
    "    num_sanity_val_steps=2,\n",
    ")\n",
    "trainer.fit(pl_scarf, datamodule) # First Training \n",
    "\n",
    "pl_scarf.set_second_phase()\n",
    "train_ds = SCARFDataset(X = X_train, Y = y_train.values, continuous_cols=continuous_cols, category_cols=category_cols, is_second_phase=True)\n",
    "valid_ds = SCARFDataset(X = X_valid, Y = y_valid.values, continuous_cols=continuous_cols, category_cols=category_cols, is_second_phase=True)\n",
    "datamodule = TS3LDataModule(train_ds, valid_ds, batch_size = batch_size, train_sampler=\"weighted\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    accelerator=device,\n",
    "    max_epochs=max_epochs,\n",
    "    num_sanity_val_steps=2,\n",
    ")\n",
    "trainer.fit(pl_scarf, datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, SequentialSampler\n",
    "\n",
    "test_ds = SCARFDataset(X_valid, continuous_cols=continuous_cols, category_cols=category_cols, is_second_phase=True)\n",
    "test_dl = DataLoader(test_ds, batch_size, shuffle=False, sampler = SequentialSampler(test_ds), num_workers=4)\n",
    "\n",
    "preds = trainer.predict(pl_scarf, test_dl)\n",
    "preds = F.softmax(torch.concat([out.cpu() for out in preds]).squeeze(),dim=1)\n",
    "\n",
    "accuracy = accuracy_score(y_valid, preds.argmax(1))\n",
    "rocuac = roc_auc_score(y_valid, preds.argmax(1))\n",
    "\n",
    "print(\"Accuracy %.2f\" % accuracy)\n",
    "print(\"rocuac %.2f\" % rocuac)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stratified K-Fold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data import preprocess_data,handle_inf_and_fillna\n",
    "import numpy as np\n",
    "\n",
    "df = X_train\n",
    "\n",
    "# df.isin([np.inf, -np.inf])은 불리언 마스크\n",
    "mask_inf = df.isin([np.inf, -np.inf])\n",
    "if mask_inf.any().any():\n",
    "    inf_cols = mask_inf.any()[mask_inf.any() == True].index\n",
    "    print(\"inf값이 존재하는 컬럼:\", inf_cols)\n",
    "\n",
    "# 최대/최소값이 너무 큰 컬럼 찾기\n",
    "for col in df.columns:\n",
    "    max_val = df[col].max()\n",
    "    min_val = df[col].min()\n",
    "    if max_val > 1e15 or min_val < -1e15:\n",
    "        print(f\"컬럼 {col} 범위가 너무 큼: min={min_val}, max={max_val}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Stratified K-Fold 설정\n",
    "n_splits = 5\n",
    "batch_size = 486\n",
    "\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=123)\n",
    "metrics = {model: [] for model in ['Custom Model']}\n",
    "k_fold_models = []\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n",
    "    print(f\"===== Fold {fold} =====\")\n",
    "\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "\n",
    "    #%% ML_Method\n",
    "    # k_fold_model = CatBoostClassifier(\n",
    "    #     iterations=700, learning_rate=0.03, depth=8, l2_leaf_reg=10,\n",
    "    #     subsample=0.8, colsample_bylevel=0.8, random_strength=10,\n",
    "    #     loss_function='Logloss', eval_metric='AUC', verbose=100\n",
    "    # )\n",
    "    \n",
    "    k_fold_model = VotingClassifier(\n",
    "        estimators=[('catboost', cat_model), ('xgboost', xgb_model), ('lightgbm', lgbm_model)],\n",
    "        voting='soft', weights=[1, 1, 1]\n",
    "    )    \n",
    "\n",
    "    \n",
    "    k_fold_model = ml_trainer(k_fold_model, X_train, y_train)\n",
    "    \n",
    "\n",
    "    #%% TabFN Model\n",
    "    # k_fold_model = tabpfn_model\n",
    "    # k_fold_model = tabfn_trainer(k_fold_model, X_train, y_train, batch_size = 256)\n",
    "    \n",
    "    # #%% TabNet\n",
    "    # k_fold_model = tabnet_model\n",
    "    # k_fold_model = tabnet_trainer(k_fold_model = k_fold_model, X_train = X_train, y_train = y_train, X_val = X_val, y_val = y_val)\n",
    "    \n",
    "\n",
    "    # #%% Ftt\n",
    "    # cat_cols, num_cols = split_cat_num_cols(X, cat_unique_threshold= 10)\n",
    "    # k_fold_model =  ftt_trainer(X_train, y_train, X_val, y_val, cat_cols, num_cols, num_cols)\n",
    "        \n",
    "    \n",
    "    k_fold_models.append(k_fold_model)\n",
    "    # 평가\n",
    "    for model_name, model in zip(metrics.keys(), [k_fold_model]):\n",
    "        metrics[model_name].append(evaluate_model(model, X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model_metrics in metrics.items():\n",
    "    avg_metrics = {metric: np.mean([fold_metric[metric] for fold_metric in model_metrics]) for metric in model_metrics[0]}\n",
    "    \n",
    "    print(f\"\\n== {model_name} Model ==\")\n",
    "    for metric, value in avg_metrics.items():\n",
    "        print(f\"{metric}: {value:.8f}\")\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "== Custom Model Model ==\n",
    "Accuracy: 0.74455055\n",
    "Precision: 0.54172313\n",
    "Recall: 0.11274579\n",
    "F1 Score: 0.18661559\n",
    "ROC AUC Score: 0.74008985\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_ID_extract(df:pd.DataFrame):\n",
    "    df['probability'] = df.groupby('ID')['probability'].transform('mean')\n",
    "    df_unique = df.drop_duplicates(subset=['ID'], keep='first').reset_index(drop=True)\n",
    "    return df_unique\n",
    "\n",
    "def max_ID_extract(df:pd.DataFrame):\n",
    "    df['probability'] = df.groupby('ID')['probability'].transform('max')\n",
    "    df_unique = df.drop_duplicates(subset=['ID'], keep='first').reset_index(drop=True)\n",
    "    return df_unique\n",
    "\n",
    "# counts = df_test['ID'].value_counts()\n",
    "# repeated_ids = counts[counts > 1].index  # 2번 이상 등장하는 ID만 뽑아냄\n",
    "\n",
    "# # 인덱스 목록으로 필터링하여, 해당 ID를 가진 행만 추출\n",
    "# df_repeated = df_test[df_test['ID'].isin(repeated_ids)]\n",
    "# df_repeated\n",
    "\n",
    "# df_mean = mean_ID_extract(df_repeated)\n",
    "# df_mean\n",
    "\n",
    "# df_max = max_ID_extract(df_repeated)\n",
    "# df_max\n",
    "\n",
    "# (df_mean['probability'] == df_max['probability']).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.EDA import vis_prob_barchart\n",
    "# 평가 지표 평균 출력\n",
    "for idx, model in enumerate(k_fold_models):\n",
    "    # df_test를 수정하지 않고, drop(columns='ID')만 해서 사용\n",
    "    pred_proba = model.predict_proba(df_test.drop(columns='ID'))[:, 1]\n",
    "    \n",
    "    # df_test에 probability 컬럼을 만들지 않고, 바로 max_ID_extract에 넣을 임시 DF를 만듦\n",
    "    temp_df = df_test[['ID']].copy()\n",
    "    temp_df['probability'] = pred_proba\n",
    "    \n",
    "    pred_proba_max = max_ID_extract(temp_df)['probability']\n",
    "    test[f'probability_{idx+1}'] = pred_proba_max\n",
    "\n",
    "# test에 probability_1 ~ probability_5 컬럼이 있을 때\n",
    "pred_main_proba = test[[\n",
    "    'probability_1',\n",
    "    'probability_2',\n",
    "    'probability_3',\n",
    "    'probability_4',\n",
    "    'probability_5'\n",
    "]].mean(axis=1)\n",
    "\n",
    "pred_main_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBMISSION_DATA_PATH = os.getenv('SUBMISSION_DATA_PATH')\n",
    "sample_submission = pd.read_csv(SUBMISSION_DATA_PATH)\n",
    "sample_submission['probability'] = pred_main_proba\n",
    "# 저장\n",
    "import datetime \n",
    "now = datetime.datetime.now()\n",
    "save_path = os.path.join(f'./log/submission/{now.strftime(\"%Y%m%d_%H%M%S\")}.csv')\n",
    "print(f\"save path : {save_path}\")\n",
    "sample_submission.to_csv(save_path, index=False)\n",
    "sample_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 확인용\n",
    "submission = pd.read_csv(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "insu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
